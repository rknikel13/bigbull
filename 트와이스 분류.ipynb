{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 분류 데이터 로딩\n",
    "root_dir = \"./twice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 명령 지정 (1)\n",
    "categories = [\"nayeon\", \"momo\", \"zzeui\", \"jungyeon\", \n",
    "                \"sana\"]\n",
    "nb_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기 지정 (2)\n",
    "image_width = 64\n",
    "image_height = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 나연\n",
      "크롤링할 갯수 입력(최대50개)50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "### 이미지 크롤링(나연)\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    " \n",
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/nayeon/nayeon' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 정연\n",
      "크롤링할 갯수 입력(최대50개)50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "### 이미지 크롤링(정연)\n",
    "\n",
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/jungyeon/jungyeon' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 모모\n",
      "크롤링할 갯수 입력(최대50개)50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/momo/momo' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 사나\n",
      "크롤링할 갯수 입력(최대50개)50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/sana/sana' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 쯔위\n",
      "크롤링할 갯수 입력(최대50개)50\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/zzeui/zzeui' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트용 이미지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 트와이스정연\n",
      "크롤링할 갯수 입력(최대50개)15\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/test_data/jungyeon/jungyeon' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 트와이스모모\n",
      "크롤링할 갯수 입력(최대50개)15\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/test_data/momo/momo' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 트와이스나연\n",
      "크롤링할 갯수 입력(최대50개)15\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/test_data/nayeon/nayeon' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 트와이스사나\n",
      "크롤링할 갯수 입력(최대50개)15\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/test_data/sana/sana' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력: 트와이스쯔위\n",
      "크롤링할 갯수 입력(최대50개)15\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "Image Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "plusUrl = input('검색어 입력: ') \n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대50개)'))\n",
    " \n",
    "url = baseUrl + quote_plus(plusUrl) # 한글 검색 자동 변환\n",
    "html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\")\n",
    "img = soup.find_all(class_='_img')\n",
    " \n",
    "n = 1\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['data-source']\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./twice/test_data/zzeui/zzeui' + str(n)+'.jpg','wb') as h: # w - write b - binary\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n += 1\n",
    "    if n > crawl_num:\n",
    "        break\n",
    "    \n",
    "    \n",
    "print('Image Crawling is done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twice/nayeon/*.jpg\n",
      "./twice/momo/*.jpg\n",
      "./twice/zzeui/*.jpg\n",
      "./twice/jungyeon/*.jpg\n",
      "./twice/sana/*.jpg\n"
     ]
    }
   ],
   "source": [
    "## 데이터 변수\n",
    "X = [] # 이미지 데이터\n",
    "Y = [] # 레이블 데이터\n",
    "\n",
    "for idx, category in enumerate(categories):\n",
    "    image_dir = root_dir + category\n",
    "    files = glob.glob(image_dir + \"/\" + \"*.jpg\")\n",
    "    print(image_dir + \"/\" +\"*.jpg\")\n",
    "\n",
    "    for i, f in enumerate(files):\n",
    "        # 이미지 로딩 (3)\n",
    "        img = Image.open(f)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_width, image_height))\n",
    "        data = np.asarray(img)\n",
    "        X.append(data)\n",
    "        Y.append(idx)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 64, 64, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터와 테스트 데이터 나누기 (4)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = (X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 파일 저장 (5)\n",
    "np.save(root_dir + \"twice.npy\", xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## 사용할 모델 라이브러리 import\n",
    "import sys, os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 초기 설정\n",
    "root_dir = \"./twice/\"\n",
    "categories = [\"nayeon\", \"jungyeon\", \"zzeui\", \"sana\", \n",
    "                \"momo\"]\n",
    "nb_classes = len(categories)\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩  (1)\n",
    "def load_dataset():\n",
    "    x_train, x_test, y_train, y_test = np.load(\"./twice/twice.npy\", allow_pickle=True)\n",
    "    x_train = x_train.astype(\"float\") / 256\n",
    "    x_test = x_test.astype(\"float\") / 256\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return  x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성  (2)\n",
    "def build_model(in_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='Same', \n",
    "                input_shape=in_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(64, 3, 3))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                    optimizer='rmsprop', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 수행하고 저장된 모델을 파일로 저장  (3)\n",
    "def model_train(x, y):\n",
    "    model = build_model(x.shape[1:])\n",
    "    model.fit(x, y, batch_size=32, epochs=30)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가하기  (4)\n",
    "def model_eval(model, x, y):\n",
    "    score = model.evaluate(x, y)\n",
    "    print('loss=', score[0])\n",
    "    print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TJ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., padding=\"Same\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\TJ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\TJ\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "187/187 [==============================] - 2s 10ms/step - loss: 2.2976 - accuracy: 0.6952\n",
      "Epoch 2/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 1.1164 - accuracy: 0.7519\n",
      "Epoch 3/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4992 - accuracy: 0.8000\n",
      "Epoch 4/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.5028 - accuracy: 0.8000\n",
      "Epoch 5/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4954 - accuracy: 0.8000\n",
      "Epoch 6/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4857 - accuracy: 0.8000\n",
      "Epoch 7/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.5060 - accuracy: 0.7979\n",
      "Epoch 8/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4733 - accuracy: 0.8011\n",
      "Epoch 9/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4837 - accuracy: 0.7989\n",
      "Epoch 10/30\n",
      "187/187 [==============================] - 2s 10ms/step - loss: 0.4484 - accuracy: 0.7989\n",
      "Epoch 11/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.4345 - accuracy: 0.8107\n",
      "Epoch 12/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.3763 - accuracy: 0.8299\n",
      "Epoch 13/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.3062 - accuracy: 0.8684\n",
      "Epoch 14/30\n",
      "187/187 [==============================] - 2s 10ms/step - loss: 0.2656 - accuracy: 0.8930\n",
      "Epoch 15/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.2840 - accuracy: 0.8930\n",
      "Epoch 16/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.1793 - accuracy: 0.9337\n",
      "Epoch 17/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.1729 - accuracy: 0.9337\n",
      "Epoch 18/30\n",
      "187/187 [==============================] - 2s 10ms/step - loss: 0.0907 - accuracy: 0.9722\n",
      "Epoch 19/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0736 - accuracy: 0.9786\n",
      "Epoch 20/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0573 - accuracy: 0.9818\n",
      "Epoch 21/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0301 - accuracy: 0.9882\n",
      "Epoch 22/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.0169 - accuracy: 0.9957\n",
      "Epoch 23/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0320 - accuracy: 0.9882\n",
      "Epoch 24/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.1431 - accuracy: 0.9561\n",
      "Epoch 25/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0369 - accuracy: 0.9861\n",
      "Epoch 26/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.0176 - accuracy: 0.9957\n",
      "Epoch 27/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "187/187 [==============================] - 2s 8ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "187/187 [==============================] - 2s 9ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "63/63 [==============================] - 0s 2ms/step\n",
      "loss= 1.4252582145115686\n",
      "accuracy= 0.7174603939056396\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 및 평가\n",
    "x_train, x_test, y_train, y_test = load_dataset()\n",
    "model = model_train(x_train, y_train)\n",
    "model_eval(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save(\"./twice/twice_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 만들어진 모델을 활용한 트와이스 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 이미지 목록 (1)\n",
    "image_files = [\"twice/test_data/nayeon/nayeon3.jpg\",\n",
    "               \"twice/test_data/jungyeon/jungyeon8.jpg\",\n",
    "               \"twice/test_data/momo/momo11.jpg\",\n",
    "               \"twice/test_data/sana/sana12.jpg\",\n",
    "               \"twice/test_data/zzeui/zzeui1.jpg\"]\n",
    "image_size = 64\n",
    "nb_classes = len(image_files)\n",
    "categories = [\"nayeon\", \"jungyeon\", \"momo\", \"sana\",\n",
    "              \"zzeui\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "files = []\n",
    "# 이미지 불러오기 (2)\n",
    "for fname in image_files:\n",
    "    img = Image.open(fname)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize((image_size, image_size))\n",
    "    in_data = np.asarray(img)\n",
    "    in_data = in_data.astype(\"float\") / 256\n",
    "    X.append(in_data)\n",
    "    files.append(fname)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파일 읽어오기  (3)\n",
    "model = load_model('./twice/twice_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 실행  (4)\n",
    "pre = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: twice/test_data/nayeon/nayeon3.jpg\n",
      "예측: [ 3 ] sana / Score 0.7945487\n",
      "입력: twice/test_data/jungyeon/jungyeon8.jpg\n",
      "예측: [ 0 ] nayeon / Score 0.99724954\n",
      "입력: twice/test_data/momo/momo11.jpg\n",
      "예측: [ 4 ] zzeui / Score 0.9997011\n",
      "입력: twice/test_data/sana/sana12.jpg\n",
      "예측: [ 1 ] jungyeon / Score 0.7253481\n",
      "입력: twice/test_data/zzeui/zzeui1.jpg\n",
      "예측: [ 0 ] nayeon / Score 0.8158291\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과 출력 (5)\n",
    "for i, p in enumerate(pre):\n",
    "    y = p.argmax()\n",
    "    print(\"입력:\", files[i])\n",
    "    print(\"예측:\", \"[\", y, \"]\", categories[y], \"/ Score\",p[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
